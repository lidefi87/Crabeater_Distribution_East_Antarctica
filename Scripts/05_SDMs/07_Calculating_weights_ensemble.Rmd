---
title: "Optimising weights for ensemble mean"
author: "Denisse Fierro Arcos"
date: "2023-12-14"
output: 
  github_document:
    toc: true
    html_preview: false
---

# Finding optimal weighting for ensemble mean
We calculated three model performance metric for each of the four models to be included in the final ensemble mean. These metrics include: the area under the the receiver operating curve (AUC ROC), the area under the precision-recall gain curve (AUC PRG) and the Pearson correlation between the model predictions and the testing dataset. AUC values give an indication of how good the model is at discriminating presences and absences, while the correlation gives us information about the agreement between the observations and the model predictions.  
  
Not all SDM algorithms performed equally well, with BRTs and RFs outperforming GAMs and Maxent in all metrics. Therefore, the contribution of each algorithm towards the mean distribution estimates should be weighted by the model performance. In this notebook, we will find the weighting scheme that produces the an ensemble mean that more closely resembles observations (i.e., the weighted ensemble mean should result in the smallest Root Mean Square Error or RMSE).  
  
## Loading libraries
  
```{r, message = F, warning = F}
library(tidyverse)
library(tidytext)
library(mgcv)
library(SDMtune)
library(randomForest)
library(gbm)
library(prg)
library(pROC)
source("useful_functions.R")
```

```{r}
knitr::opts_chunk$set(fig.path = "figures/") 
```


## Loading model evaluation metrics
These metrics were calculated for each SDM algorithm and compiled into a single file.  
  
```{r}
mod_eval_path <- "../../SDM_outputs/model_evaluation.csv"
model_eval <- read_csv(mod_eval_path) 

#Check data
model_eval
```
  
### Plotting model performance metrics
Before attempting to find the best weighting scheme, we will visualise the data.  
  
```{r}
model_eval %>% 
  #Rearrange data to facilitate plotting
  pivot_longer(c(auc_roc:pear_cor), names_to = "metric", 
               values_to = "value") %>% 
  #Renaming models to ensure figure labels show correctly
  mutate(model = case_when(str_detect(model, "Random") ~ "RF",
                           str_detect(model, "Trees") ~ "BRT", 
                           T ~ model),
         #Turning column to factor
         model = factor(model),
         #Renaming metrics to ensure figure labels show correctly
         metric = case_when(str_detect(metric, "auc") ~ 
                              str_to_upper(str_replace(metric, "_", " ")),
                            T ~ "Pearson correlation")) %>%
  #Plot metrics as columns
  ggplot(aes(x = reorder_within(model, desc(value), metric),
             y = value))+
  geom_col(position = "dodge", aes(fill = env_trained))+
  scale_x_reordered()+
  #Divide plots by SDM algorithms and source of environmental data used for 
  #training model
  facet_grid(env_trained~metric, scales = "free_x")+
  theme_bw()+
  scale_fill_manual(values = c("#eecc66", "#6699cc", "#004488"),
                    labels = c("ACCESS-OM2-01 (full set)",
                               "ACCESS-OM2-01 (reduced set)",
                               "Observations"))+
  guides(fill = guide_legend(title = "Environmental dataset used for training",
                             title.position = "top", title.hjust = 0.5))+
  #Improving plot
  theme(axis.title = element_blank(), panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank(), strip.text.y = element_blank(),
        legend.position = "bottom", panel.spacing.y = unit(0.35, "cm"))
```
  
Regardless of the source of the environmental data used to train the model, we can see the same pattern in all of them, so we will use the smaller ACCESS-OM2-01 environmental dataset to check the best weighting scheme.  
  
We can also see that AUC PRG and AUC ROC also show the same pattern, with highest values for Random Forest (RF) and smallest values for GAMs. However, the Pearson correlation is slightly different, with the highest values for Random Forest (RF) and smallest values for Maxent. This is why we will use AUC PRG and the Pearson correlation to test the best combination of weights.    
  
## Loading testing data
  
```{r}
#Loading data
mod_match_obs <- read_csv(
  file.path("../../Environmental_Data/",
            "mod-match-obs_env_pres_bg_20x_Indian_weaning.csv")) %>% 
  #Setting month as factor and ordered factor
  mutate(month = as.factor(month))

#Preparing training data and testing data for GAM
mod_match_obs <- prep_data(mod_match_obs, "month", split = T)

#Applying SWD format for all other algorithms
mod_data_sdm <- mod_match_obs$baked_test %>% 
  select(!year) %>% 
  sdm_format() 
```
  
## Loading models and predicting presence (ACCESS-OM2-01 matching observations)
  
```{r}
#Loading models
#GAM
gam_mod <- readRDS(
  "../../SDM_outputs/GAM/Mod_match_obs/best_GAM_mod_match_obs.rds")
#Maxent
maxent_mod <- readRDS(
  file.path("../../SDM_outputs/Maxent/Mod_match_obs/reduced_Maxent_model",
            "best_red_maxent_model.rds"))
#Random Forest
rf_mod <- readRDS(
  file.path("../../SDM_outputs/RandomForest/Mod_match_obs",
            "reduced_RF_mod_match_obs.rds"))
#Boosted Regression Trees
brt_mod <- readRDS(
  file.path("../../SDM_outputs/BoostedRegressionTrees",
            "Mod_match_obs/best_BRT_mod_match_obs.rds"))

#Predictions
#GAM
gam_pred <- predict(gam_mod, mod_match_obs$baked_test, type = "response")
#Maxent
maxent_pred <- predict(maxent_mod, mod_data_sdm@data, type = "cloglog")
#Random Forest
rf_pred <- predict(rf_mod, mod_data_sdm@data, type = "response")
#Boosted Regression Trees
brt_pred <- predict(brt_mod, mod_data_sdm@data, type = "response")

#Getting all predictions into a single tibble
preds <- tibble(gam = as.numeric(gam_pred), maxent = maxent_pred, rf = rf_pred, 
                brt = brt_pred) 
```
  
## Normalising weights

```{r}
#Getting relevant weights
weights <- model_eval %>% 
  filter(env_trained == "mod_match_obs")

#Normalising weights
weights <- weights %>% 
  ungroup() %>%
  mutate(auc_norm_weights = (auc_prg - min(auc_prg))/
           (max(auc_prg)-min(auc_prg)),
         pear_norm_weights = (pear_cor - min(pear_cor))/
           (max(pear_cor)-min(pear_cor))) %>% 
  #Ensuring values add up to 1
  mutate(auc_norm_weights = auc_norm_weights/sum(auc_norm_weights),
         pear_norm_weights = pear_norm_weights/sum(pear_norm_weights))
```

## Calculating ensemble mean and weighted ensemble mean
We will calculate the RMSE value for the unweighted ensemble mean and weighted ensemble means. We will also use two types of weights: raw and normalised AUC PRG and Pearson correlation values.  
  
```{r}
preds <- preds %>% 
  rowwise() %>%
  #Calculating ensemble mean (unweighted)
  mutate(ensemble_mean =  mean(c_across(gam:brt)),
         #Weighted ensemble means
         auc_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$auc_prg),
         auc_norm_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$auc_norm_weights),
         pear_weighted_ensemble_mean =
           weighted.mean(c_across(gam:brt), w = weights$pear_cor),
         pear_norm_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$pear_norm_weights))

#Calculating performance metrics
auc_roc <- roc(mod_match_obs$baked_test$presence, 
               preds$pear_norm_weighted_ensemble_mean) %>% 
  auc() %>% 
  as.numeric()

auc_prg <- create_prg_curve(mod_match_obs$baked_test$presence, 
                            preds$pear_norm_weighted_ensemble_mean) %>% 
  calc_auprg()

cor <- cor(preds$pear_norm_weighted_ensemble_mean, 
           mod_match_obs$baked_test$presence)

#Adding to model evaluation data frame
model_eval <- model_eval %>% 
  bind_rows(data.frame(model = "WeightedEnsemble", 
                       env_trained = "mod_match_obs", 
                       auc_roc = auc_roc, auc_prg = auc_prg, pear_cor = cor,
                       pear_norm_weights = NA))

#Threshold calculation
thr_ensemble <- thresholds_adap_ensemble(mod_match_obs$baked_test,
                         preds$pear_norm_weighted_ensemble_mean) %>% 
  mutate(env_trained = "mod_match_obs")


#Checking results
head(preds)
```


```{r}
#Calculating RMSE values
preds %>% 
  ungroup() %>% 
  #Apply to all ensemble mean columns (weighted and unweighted)
  summarise(across(gam:pear_norm_weighted_ensemble_mean,
                ~ sqrt(mean((mod_match_obs$baked_test$presence - .x)^2)))) %>% 
  #Reorganise table to ease interpretation
  pivot_longer(everything(), names_to = "model_weighting", 
               values_to = "RMSE") %>% 
  #Arrange by RMSE values
  arrange(RMSE)
```
  
The smallest RMSE was estimated when normalised Pearson correlation values were applied as weights. We will use these weights in the final ensemble mean. We will now calculate the normalised Pearson values and save the weights so we can easily apply them to the final result.  
  
## Saving weights
  
```{r, eval = F}
model_eval %>% 
  group_by(env_trained) %>% 
  #Normalising
  mutate(pear_norm_weights = (pear_cor - min(pear_cor))/
           (max(pear_cor)-min(pear_cor))) %>% 
  #Ensuring values add up to 1
  mutate(pear_norm_weights = pear_norm_weights/sum(pear_norm_weights)) %>% 
  #Saving results
  write_csv(mod_eval_path)
```
  
We will now calculate the RMSE values for the full ACCESS-OM2-01 set and observations.  
  
## Loading models and predicting presence (ACCESS-OM2-01 full set)
  
```{r}
#Loading models
#GAM
gam_mod <- readRDS("../../SDM_outputs/GAM/Mod_full/best_GAM_mod_full.rds")
#Maxent
maxent_mod <- readRDS(
  "../../SDM_outputs/Maxent/Mod_full/initial_Maxent_model/model.Rds")
#Random Forest
rf_mod <- readRDS("../../SDM_outputs/RandomForest/Mod_full/model.Rds")
#Boosted Regression Trees
brt_mod <- readRDS(
  "../../SDM_outputs/BoostedRegressionTrees/Mod_full/model.Rds")

#Loading data
mod_full <- read_csv(
  "../../Environmental_Data/model_env_pres_bg_20x_Indian_weaning.csv") %>% 
  #Setting month as factor and ordered factor
  mutate(month = as.factor(month))

#Preparing training data and testing data for GAM
mod_full <- prep_data(mod_full, "month", split = T)

#Applying SWD format for all other algorithms
mod_full_data_sdm <- mod_full$baked_test %>% 
  select(!year) %>% 
  sdm_format() 

#Predictions
#GAM
gam_pred <- predict(gam_mod, mod_full$baked_test, type = "response")
#Maxent
maxent_pred <- predict(maxent_mod, mod_full_data_sdm@data, type = "cloglog")
#Random Forest
rf_pred <- predict(rf_mod, mod_full_data_sdm@data, type = "response")
#Boosted Regression Trees
brt_pred <- predict(brt_mod, mod_full_data_sdm@data, type = "response")

#Getting all predictions into a single tibble
preds <- tibble(gam = as.numeric(gam_pred), maxent = maxent_pred, rf = rf_pred, 
                brt = brt_pred) 
```
  
### Weights
  
```{r}
#Getting relevant weights
weights <- model_eval %>% 
  filter(env_trained == "full_access")

#Normalising weights
weights <- weights %>% 
  ungroup() %>%
  mutate(auc_norm_weights = (auc_prg - min(auc_prg))/
           (max(auc_prg)-min(auc_prg)),
         pear_norm_weights = (pear_cor - min(pear_cor))/
           (max(pear_cor)-min(pear_cor))) %>% 
  #Ensuring values add up to 1
  mutate(auc_norm_weights = auc_norm_weights/sum(auc_norm_weights),
         pear_norm_weights = pear_norm_weights/sum(pear_norm_weights))
```
  
### Predictions
  
```{r}
preds <- preds %>% 
  rowwise() %>%
  #Calculating ensemble mean (unweighted)
  mutate(ensemble_mean =  mean(c_across(gam:brt)),
         #Weighted ensemble means
         auc_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$auc_prg),
         auc_norm_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$auc_norm_weights),
         pear_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$pear_cor),
         pear_norm_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$pear_norm_weights))

#Calculating performance metrics
auc_roc <- roc(mod_full$baked_test$presence, 
               preds$pear_norm_weighted_ensemble_mean) %>% 
  auc() %>% 
  as.numeric()

auc_prg <- create_prg_curve(mod_full$baked_test$presence, 
                            preds$pear_norm_weighted_ensemble_mean) %>% 
  calc_auprg()

cor <- cor(preds$pear_norm_weighted_ensemble_mean, 
           mod_full$baked_test$presence)

#Adding to model evaluation data frame
model_eval <- model_eval %>% 
  bind_rows(data.frame(model = "WeightedEnsemble", 
                       env_trained = "full_access", 
                       auc_roc = auc_roc, auc_prg = auc_prg, pear_cor = cor,
                       pear_norm_weights = NA))

#Threshold calculation
thr_ensemble <- thresholds_adap_ensemble(mod_full$baked_test,
                                         preds$pear_norm_weighted_ensemble_mean) %>% 
  mutate(env_trained = "full_access") %>% 
  bind_rows(thr_ensemble)

#Calculating RMSE values
preds %>% 
  ungroup() %>% 
  #Apply to all ensemble mean columns (weighted and unweighted)
  summarise(across(gam:pear_norm_weighted_ensemble_mean,
                ~ sqrt(mean((mod_full$baked_test$presence - .x)^2)))) %>% 
  #Reorganise table to ease interpretation
  pivot_longer(everything(), names_to = "model_weighting", 
               values_to = "RMSE") %>% 
  #Arrange by RMSE values
  arrange(RMSE)
```
  
## Loading models and predicting presence (Observations)
  
```{r}
#Loading models
#GAM
gam_mod <- readRDS("../../SDM_outputs/GAM/Obs/best_GAM_obs.rds")
#Maxent
maxent_mod <- readRDS(
  file.path("../../SDM_outputs/Maxent/Obs/reduced_Maxent_model",
            "best_red_maxent_model.rds"))
#Random Forest
rf_mod <- readRDS("../../SDM_outputs/RandomForest/Obs/model.Rds")
#Boosted Regression Trees
brt_mod <- readRDS("../../SDM_outputs/BoostedRegressionTrees/Obs/model.Rds")

#Loading data
obs <- read_csv(
  "../../Environmental_Data/obs_env_pres_bg_20x_Indian_weaning.csv") %>% 
  #Setting month as factor and ordered factor
  mutate(month = as.factor(month))

#Preparing training data and testing data for GAM
obs <- prep_data(obs, "month", split = T)

#Applying SWD format for all other algorithms
obs_data_sdm <- obs$baked_test %>% 
  select(!year) %>% 
  sdm_format() 

#Predictions
#GAM
gam_pred <- predict(gam_mod, obs$baked_test, type = "response")
#Maxent
maxent_pred <- predict(maxent_mod, obs_data_sdm@data, type = "cloglog")
#Random Forest
rf_pred <- predict(rf_mod, obs_data_sdm@data, type = "response")
#Boosted Regression Trees
brt_pred <- predict(brt_mod, obs_data_sdm@data, type = "response")

#Getting all predictions into a single tibble
preds <- tibble(gam = as.numeric(gam_pred), maxent = maxent_pred, rf = rf_pred, 
                brt = brt_pred) 
```
  
### Weights
  
```{r}
#Getting relevant weights
weights <- model_eval %>% 
  filter(env_trained == "observations")

#Normalising weights
weights <- weights %>% 
  ungroup() %>%
  mutate(auc_norm_weights = (auc_prg - min(auc_prg))/
           (max(auc_prg)-min(auc_prg)),
         pear_norm_weights = (pear_cor - min(pear_cor))/
           (max(pear_cor)-min(pear_cor))) %>% 
  #Ensuring values add up to 1
  mutate(auc_norm_weights = auc_norm_weights/sum(auc_norm_weights),
         pear_norm_weights = pear_norm_weights/sum(pear_norm_weights))
```
  
### Predictions
  
```{r}
preds <- preds %>% 
  rowwise() %>%
  #Calculating ensemble mean (unweighted)
  mutate(ensemble_mean =  mean(c_across(gam:brt)),
         #Weighted ensemble means
         auc_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$auc_prg),
         auc_norm_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$auc_norm_weights),
         pear_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$pear_cor),
         pear_norm_weighted_ensemble_mean = 
           weighted.mean(c_across(gam:brt), w = weights$pear_norm_weights))

#Calculating performance metrics
auc_roc <- roc(obs$baked_test$presence, 
               preds$pear_norm_weighted_ensemble_mean) %>% 
  auc() %>% 
  as.numeric()

auc_prg <- create_prg_curve(obs$baked_test$presence, 
                            preds$pear_norm_weighted_ensemble_mean) %>% 
  calc_auprg()

cor <- cor(preds$pear_norm_weighted_ensemble_mean, 
           obs$baked_test$presence)

#Adding to model evaluation data frame
model_eval <- model_eval %>% 
  bind_rows(data.frame(model = "WeightedEnsemble", 
                       env_trained = "observations", 
                       auc_roc = auc_roc, auc_prg = auc_prg, pear_cor = cor,
                       pear_norm_weights = NA))


#Threshold calculation
thr_ensemble <- thresholds_adap_ensemble(obs$baked_test,
                                         preds$pear_norm_weighted_ensemble_mean) %>% 
  mutate(env_trained = "observations") %>% 
  bind_rows(thr_ensemble)

#Calculating RMSE values
preds %>% 
  ungroup() %>% 
  #Apply to all ensemble mean columns (weighted and unweighted)
  summarise(across(gam:pear_norm_weighted_ensemble_mean,
                ~ sqrt(mean((obs$baked_test$presence - .x)^2)))) %>% 
  #Reorganise table to ease interpretation
  pivot_longer(everything(), names_to = "model_weighting",
               values_to = "RMSE") %>% 
  #Arrange by RMSE values
  arrange(RMSE)
```
  
Saving weighted ensemble results.  
  
```{r, eval=FALSE}
model_eval %>% 
  write_csv("../../SDM_outputs/model_evaluation_plus_ensemble.csv")
```

